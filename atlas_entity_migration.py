import requests
import json
from pymongo import MongoClient
from datetime import datetime
import logging

ATLAS_URL = "http://172.26.0.2:21000"  # VÃ©rifiez que votre Atlas est accessible
ATLAS_USER = "admin"                    # VÃ©rifiez les credentials
ATLAS_PASS = "ensias123"          # VÃ©rifiez les credentials

# Configuration MongoDB - Ã€ AJUSTER selon votre environnement
MONGO_URI = 'mongodb://mongodb:27017/' 

# Configuration logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class AtlasMetadataGovernance:
    def __init__(self):
        self.atlas_url = ATLAS_URL
        self.auth = (ATLAS_USER, ATLAS_PASS)
        self.mongo_client = MongoClient(MONGO_URI)
        self.metadata_db = self.mongo_client['metadata_validation_db']
        
        # Test de connectivitÃ© initial
        self._test_connections()
        
    def _test_connections(self):
        """Tester les connexions Atlas et MongoDB"""
        logger.info("ğŸ” Test des connexions...")
        
        # Test MongoDB
        try:
            self.mongo_client.admin.command('ping')
            logger.info("âœ… MongoDB connectÃ©")
        except Exception as e:
            logger.error(f"âŒ MongoDB non accessible: {e}")
            raise
        
        # Test Atlas
        


    def get_hive_table_entity(self, table_name):  
     """RÃ©cupÃ©rer l'entitÃ© table Hive et ses colonnes"""  
     search_url = f"{self.atlas_url}/api/atlas/v2/search/dsl"  
     dsl_query = f"hive_table where name='{table_name}'"  
      
     response = requests.get(  
        search_url,  
        auth=self.auth,  
        params={'query': dsl_query}  
     )  
     logger.info(f"Recherche table {table_name}: {response.status_code}")  

     if response.status_code == 200:  
        entities = response.json().get('entities', [])  
        logger.info(f"EntitÃ©s trouvÃ©es: {len(entities)}")  

        if entities:  
            return entities[0]['guid']  
     return None  
  
    def get_table_columns(self, table_guid):  
     """RÃ©cupÃ©rer toutes les colonnes de la table"""  
     entity_url = f"{self.atlas_url}/api/atlas/v2/entity/guid/{table_guid}"  
      
     response = requests.get(  
        entity_url,  
        auth=self.auth,  
        params={'ignoreRelationships': 'false'}  
     )  
      
     if response.status_code == 200:  
        entity = response.json()['entity']  
        columns = entity.get('relationshipAttributes', {}).get('columns', [])  
          
        column_info = []  
        for col in columns:  
            column_info.append({  
                'guid': col['guid'],  
                'name': col['displayText'],  
                'type': col['typeName']  
            })  
        return column_info  
     return []
    

    def assign_glossary_term_to_column(self, column_guid, term_guid):  
     """Assigner un terme de glossaire Ã  une colonne Hive"""  
     assign_url = f"{self.atlas_url}/api/atlas/v2/entity/guid/{column_guid}/meanings"  
      
     payload = [{  
        "termGuid": term_guid,  
        "relationGuid": None  
     }]  
      
     response = requests.post(  
        assign_url,  
        auth=self.auth,  
        headers={'Content-Type': 'application/json'},  
        json=payload  
     )  
      

     if response.status_code != 200:  
        logger.error(f"Erreur assignation: {response.status_code} - {response.text}")  
      
     return response.status_code == 200  
  
    def get_glossary_term_guid(self, term_name):  
     """RÃ©cupÃ©rer le GUID d'un terme du glossaire"""  
     search_url = f"{self.atlas_url}/api/atlas/v2/search/basic"  
      
     response = requests.get(  
        search_url,  
        auth=self.auth,  
        params={  
            'query': term_name,  
            'typeName': 'AtlasGlossaryTerm',  
            'excludeDeletedEntities': 'true'  
        }  
     )  
     logger.info(f"Recherche terme {term_name}: {response.status_code}")  

     if response.status_code == 200:  
        entities = response.json().get('entities', [])  
        logger.info(f"Termes trouvÃ©s: {len(entities)}")  
        for entity in entities:  
            if entity['displayText'] == term_name:  
                return entity['guid']  
     return None
    

    def create_column_term_mapping(self):  
     """CrÃ©er mapping entre colonnes Hive et termes du glossaire"""  
     # Utiliser vos mÃ©tadonnÃ©es validÃ©es existantes  
     enriched_metadata = self.metadata_db['enriched_metadata']  
     validated_metadata = list(enriched_metadata.find({"validation_status": "validated"}))  
      
     mapping = {}  
      
     for metadata in validated_metadata:  
        column_name = metadata['column_name']  
        entity_types = metadata.get('entity_types', [])  
        rgpd_category = metadata.get('recommended_rgpd_category')  
          
        # CrÃ©er le nom du terme comme dans votre mÃ©thode create_validated_metadata_terms  
        term_name = f"{column_name.upper()}_TERM"  
          
        mapping[column_name] = term_name  
          
        logger.info(f"Mapping crÃ©Ã©: {column_name} â†’ {term_name}")  
      
     return mapping


    def get_glossary_term_guid(self, term_name):  
     """RÃ©cupÃ©rer le GUID d'un terme du glossaire avec sÃ©lection prÃ©cise"""  
     search_url = f"{self.atlas_url}/api/atlas/v2/search/basic"  
      
     response = requests.get(  
        search_url,  
        auth=self.auth,  
        params={  
            'query': term_name,  
            'typeName': 'AtlasGlossaryTerm',  
            'excludeDeletedEntities': 'true'  
        }  
     )  
      
     if response.status_code == 200:  
        entities = response.json().get('entities', [])  
          
        # Filtrer par glossaire exact et nom exact  
        for entity in entities:  
            # VÃ©rifier que le terme appartient au bon glossaire  
            if (entity['displayText'] == term_name and   
                entity.get('attributes', {}).get('anchor', {}).get('glossaryGuid') == self.current_glossary_guid):  
                return entity['guid']  
          
        # Fallback : sÃ©lection par nom exact seulement  
        for entity in entities:  
            if entity['displayText'] == term_name:  
                return entity['guid']  
      
     return None
    
    def preview_sync_data(self):
        """PrÃ©visualiser les donnÃ©es qui seront synchronisÃ©es"""
        enriched_metadata = self.metadata_db['enriched_metadata']
        
        # Statistiques
        total_metadata = enriched_metadata.count_documents({})
        validated_metadata = enriched_metadata.count_documents({"validation_status": "validated"})
        pending_metadata = enriched_metadata.count_documents({"validation_status": "pending"})
        
        # CatÃ©gories et niveaux uniques
        categories = enriched_metadata.distinct('recommended_rgpd_category')
        sensitivity_levels = enriched_metadata.distinct('recommended_sensitivity_level')
        
        preview = {
            "total_metadata": total_metadata,
            "validated_metadata": validated_metadata,
            "pending_metadata": pending_metadata,
            "rgpd_categories": [cat for cat in categories if cat],
            "sensitivity_levels": [level for level in sensitivity_levels if level],
            "will_sync": validated_metadata > 0
        }
        
        logger.info("ğŸ“Š PRÃ‰VISUALISATION DE LA SYNCHRONISATION")
        logger.info(f"ğŸ“ Total mÃ©tadonnÃ©es: {total_metadata}")
        logger.info(f"âœ… MÃ©tadonnÃ©es validÃ©es (Ã  synchroniser): {validated_metadata}")
        logger.info(f"â³ MÃ©tadonnÃ©es en attente: {pending_metadata}")
        logger.info(f"ğŸ“‚ CatÃ©gories RGPD: {preview['rgpd_categories']}")
        logger.info(f"ğŸ”’ Niveaux de sensibilitÃ©: {preview['sensitivity_levels']}")
        
        if validated_metadata == 0:
            logger.warning("âš ï¸  AUCUNE MÃ‰TADONNÃ‰E VALIDÃ‰E - Rien ne sera synchronisÃ©!")
            
        return preview
        
        # Test de connectivitÃ© initial
        self._test_connections()
        
    def create_business_glossary(self):
        """CrÃ©er le glossaire mÃ©tier principal"""
        glossary_data = {
            "name": "Data_Governance_Glossary11",
            "shortDescription": "Glossaire mÃ©tier pour la gouvernance des donnÃ©es",
            "longDescription": "Glossaire centralisÃ© contenant toutes les mÃ©tadonnÃ©es validÃ©es par les data stewards, enrichies avec les recommandations IA et conformes aux exigences RGPD"
        }
        
        response = requests.post(
            f"{self.atlas_url}/api/atlas/v2/glossary",
            json=glossary_data,
            auth=self.auth,
            timeout=(30, 60)
        )
        
        if response.status_code == 200:  
              glossary_guid = response.json()['guid']  
              self.current_glossary_guid = glossary_guid  # Stocker pour la sÃ©lection des termes  
              logger.info("âœ“ Glossaire mÃ©tier crÃ©Ã© avec succÃ¨s")  
              return glossary_guid
        else:
            logger.error(f"âœ— Erreur crÃ©ation glossaire: {response.text}")
            return {"success": False, "error": "Ã‰chec "}

    def extract_rgpd_categories_from_db(self):
        """Extraire les catÃ©gories RGPD rÃ©elles depuis la base"""
        enriched_metadata = self.metadata_db['enriched_metadata']
        categories = enriched_metadata.distinct('recommended_rgpd_category')
        categories = [cat for cat in categories if cat and cat.strip()]
        logger.info(f"CatÃ©gories RGPD dÃ©tectÃ©es: {categories}")
        return categories

    def create_rgpd_categories(self, glossary_guid):
        """CrÃ©er les catÃ©gories RGPD basÃ©es sur les donnÃ©es rÃ©elles"""
        real_categories = self.extract_rgpd_categories_from_db()
        category_guids = {}
        
        # Mapping des descriptions mÃ©tier
        category_descriptions = {
            "DonnÃ©es d'identification": "Informations permettant d'identifier directement ou indirectement une personne physique",
            "DonnÃ©es financiÃ¨res": "Informations bancaires, financiÃ¨res et de paiement",
            "DonnÃ©es de contact": "Informations de contact et de communication",
            "DonnÃ©es de localisation": "Informations gÃ©ographiques et d'adresse",
            "DonnÃ©es temporelles": "Informations de date, heure et temporelles",
            "DonnÃ©es de santÃ©": "Informations mÃ©dicales et de santÃ©",
            "DonnÃ©es biomÃ©triques": "DonnÃ©es biomÃ©triques d'identification",
            "DonnÃ©es de comportement": "DonnÃ©es de navigation et comportementales"
        }
        
        for category in real_categories:
            cat_data = {
                "name": category,
                "shortDescription": category_descriptions.get(category, f"CatÃ©gorie RGPD: {category}"),
                "longDescription": f"CatÃ©gorie de donnÃ©es personnelles selon le RGPD: {category}. Gestion automatisÃ©e avec validation data steward.",
                "anchor": {"glossaryGuid": glossary_guid}
            }
            
            response = requests.post(
                f"{self.atlas_url}/api/atlas/v2/glossary/category",
                json=cat_data,
                auth=self.auth
            )
            
            if response.status_code == 200:
                category_guids[category] = response.json()['guid']
                logger.info(f"âœ“ CatÃ©gorie RGPD crÃ©Ã©e: {category}")
            else:
                logger.error(f"âœ— Erreur catÃ©gorie {category}: {response.text}")
        
        return category_guids

    def create_sensitivity_classifications(self):
        """CrÃ©er les classifications de sensibilitÃ© basÃ©es sur les donnÃ©es rÃ©elles"""
        enriched_metadata = self.metadata_db['enriched_metadata']
        sensitivity_levels = enriched_metadata.distinct('recommended_sensitivity_level')
        sensitivity_levels = [level for level in sensitivity_levels if level]
        
        logger.info(f"Niveaux de sensibilitÃ© dÃ©tectÃ©s: {sensitivity_levels}")
        
        # Mapping des attributs mÃ©tier pour chaque niveau
        sensitivity_mapping = {
            "PUBLIC": {"risk_level": "LOW", "retention_period": "UNLIMITED", "access_level": "PUBLIC"},
            "INTERNAL": {"risk_level": "LOW", "retention_period": "7_YEARS", "access_level": "INTERNAL"},
            "CONFIDENTIAL": {"risk_level": "MEDIUM", "retention_period": "5_YEARS", "access_level": "RESTRICTED"},
            "PERSONAL_DATA": {"risk_level": "HIGH", "retention_period": "2_YEARS", "access_level": "CONTROLLED"},
            "RESTRICTED": {"risk_level": "CRITICAL", "retention_period": "1_YEAR", "access_level": "HIGHLY_RESTRICTED"}
        }
        
        classification_defs = []
        
        for level in sensitivity_levels:
            attrs = sensitivity_mapping.get(level, {"risk_level": "MEDIUM", "retention_period": "3_YEARS", "access_level": "RESTRICTED"})
            
            classification_def = {
                "name": f"DataSensitivity_{level}",
                "description": f"Classification de sensibilitÃ© des donnÃ©es: {level}",
                "attributeDefs": [
                    {"name": "sensitivity_level", "typeName": "string", "isOptional": False},
                    {"name": "risk_level", "typeName": "string", "isOptional": True},
                    {"name": "retention_period", "typeName": "string", "isOptional": True},
                    {"name": "access_level", "typeName": "string", "isOptional": True},
                    {"name": "rgpd_compliant", "typeName": "boolean", "isOptional": True},
                    {"name": "data_steward", "typeName": "string", "isOptional": True},
                    {"name": "validation_date", "typeName": "date", "isOptional": True}
                ]
            }
            classification_defs.append(classification_def)
        
        if classification_defs:
            classification_batch = {"classificationDefs": classification_defs}
            
            response = requests.post(
                f"{self.atlas_url}/api/atlas/v2/types/typedefs",
                json=classification_batch,
                auth=self.auth
            )
            
            if response.status_code == 200:
                logger.info(f"âœ“ {len(classification_defs)} classifications de sensibilitÃ© crÃ©Ã©es")
                {"success": True, "nonerror": "non echec"}
            else:
                logger.error(f"âœ— Erreur crÃ©ation classifications: {response.text}")
                return {"success": False, "error": "Ã‰chec crÃ©ation classifications"}
        
        return {"success": False, "error": "Ã‰chec "}


    def automate_hive_glossary_assignment(self, table_name="entites_marocaines"):  
     """Workflow principal d'assignation automatique"""  
     logger.info(f"ğŸš€ DÃ©but assignation automatique pour table: {table_name}")  
      
     try:  
        # 1. RÃ©cupÃ©rer la table Hive  
        table_guid = self.get_hive_table_entity(table_name)  
        if not table_guid:  
            logger.error(f"âŒ Table {table_name} non trouvÃ©e dans Atlas")  
            return {"success": False, "error": "Table non trouvÃ©e"}  
          
        # 2. RÃ©cupÃ©rer les colonnes  
        columns = self.get_table_columns(table_guid)  
        logger.info(f"ğŸ“‹ Colonnes trouvÃ©es: {[col['name'] for col in columns]}")  
          
        # 3. CrÃ©er le mapping colonnes â†’ termes  
        column_term_mapping = self.create_column_term_mapping()  
          
        # 4. Assigner les termes aux colonnes  
        assigned_count = 0  
        for column in columns:  
            column_name = column['name']  
              
            if column_name in column_term_mapping:  
                term_name = column_term_mapping[column_name]  
                  
                # RÃ©cupÃ©rer le GUID du terme  
                term_guid = self.get_glossary_term_guid(term_name)  
                  
                if term_guid:  
                    success = self.assign_glossary_term_to_column(column['guid'], term_guid)  
                      
                    if success:  
                        logger.info(f"âœ… Terme '{term_name}' assignÃ© Ã  colonne '{column_name}'")  
                        assigned_count += 1  
                    else:  
                        logger.error(f"âŒ Ã‰chec assignation pour '{column_name}'")  
                else:  
                    logger.warning(f"âš ï¸  Terme '{term_name}' non trouvÃ© dans glossaire")  
            else:  
                logger.info(f"â„¹ï¸  Pas de mapping pour colonne '{column_name}'")  
          
        return {  
            "success": True,  
            "table_guid": table_guid,  
            "columns_processed": len(columns),  
            "terms_assigned": assigned_count  
        }  
          
     except Exception as e:  
        logger.error(f"âŒ Erreur assignation: {str(e)}")  
        return {"success": False, "error": str(e)}
    



    def create_validated_metadata_terms(self, glossary_guid, category_guids):
        """CrÃ©er les termes du glossaire Ã  partir des mÃ©tadonnÃ©es validÃ©es"""
        enriched_metadata = self.metadata_db['enriched_metadata']
        
        # RÃ©cupÃ©rer uniquement les mÃ©tadonnÃ©es validÃ©es
        validated_metadata = list(enriched_metadata.find({"validation_status": "validated"}))
        logger.info(f"MÃ©tadonnÃ©es validÃ©es Ã  synchroniser: {len(validated_metadata)}")
        
        synced_terms = 0
        
        for metadata in validated_metadata:
            column_name = metadata['column_name']
            job_id = metadata['job_id']
            
            # CrÃ©er un nom de terme unique et mÃ©tier
            term_name = f"{column_name.upper()}_TERM"
            qualified_name = f"datagovernance.{column_name}_{job_id}@production"
            
            # PrÃ©parer les attributs mÃ©tier
            attributes = {
                "source_column": column_name,
                "source_dataset": job_id,
                "entity_types": metadata.get('entity_types', []),
                "total_entities": metadata.get('total_entities', 0),
                "sensitivity_level": metadata.get('recommended_sensitivity_level'),
                "rgpd_category": metadata.get('recommended_rgpd_category'),
                "ranger_policy": metadata.get('recommended_ranger_policy'),
                "validation_date": datetime.now().isoformat(),
                "data_quality_score": self._calculate_data_quality_score(metadata),
                "business_owner": "Data Steward",
                "technical_owner": "Data Engineering Team"
            }
            
            # Obtenir la catÃ©gorie RGPD
            rgpd_category = metadata.get('recommended_rgpd_category')
            category_guid = category_guids.get(rgpd_category)
            
            # PrÃ©parer les classifications
            sensitivity_level = metadata.get('recommended_sensitivity_level')
            classifications = []
            if sensitivity_level:
                classifications.append({
                    "typeName": f"DataSensitivity_{sensitivity_level}",
                    "attributes": {
                        "sensitivity_level": sensitivity_level,
                        "rgpd_compliant": True,
                        "data_steward": "Validated",
                        "validation_date": datetime.now().isoformat()
                    }
                })
            
            term_data = {
                "name": term_name,
                "qualifiedName": qualified_name,
                "shortDescription": f"Attribut mÃ©tier validÃ©: {column_name}",
                "longDescription": self._generate_business_description(metadata),
                "anchor": {"glossaryGuid": glossary_guid}
               
            }
            
            # Ajouter la catÃ©gorie RGPD si disponible
            if category_guid:
                term_data["categories"] = [{"categoryGuid": category_guid}]
            
            # CrÃ©er le terme dans Atlas
            response = requests.post(
                f"{self.atlas_url}/api/atlas/v2/glossary/term",
                json=term_data,
                auth=self.auth
            )
            
            if response.status_code == 200:
                logger.info(f"âœ“ Terme mÃ©tier synchronisÃ©: {term_name}")
                synced_terms += 1
            else:
                logger.error(f"âœ— Erreur terme {term_name}: {response.text}")
        
        return synced_terms

    def _calculate_data_quality_score(self, metadata):
        """Calculer un score de qualitÃ© des donnÃ©es"""
        score = 0
        
        # PrÃ©sence d'entitÃ©s dÃ©tectÃ©es
        if metadata.get('total_entities', 0) > 0:
            score += 30
        
        # DiversitÃ© des types d'entitÃ©s
        entity_types = metadata.get('entity_types', [])
        if len(entity_types) > 0:
            score += 20
        
        # Validation par data steward
        if metadata.get('validation_status') == 'validated':
            score += 40
        
        # PrÃ©sence d'Ã©chantillons
        if metadata.get('sample_values') and len(metadata.get('sample_values', [])) > 0:
            score += 10
        
        return min(score, 100)

    def _generate_business_description(self, metadata):
        """GÃ©nÃ©rer une description mÃ©tier riche"""
        column_name = metadata['column_name']
        entity_types = metadata.get('entity_types', [])
        sensitivity = metadata.get('recommended_sensitivity_level', 'INTERNAL')
        rgpd_category = metadata.get('recommended_rgpd_category', 'Non classifiÃ©')
        total_entities = metadata.get('total_entities', 0)
        
        description = f"""
ATTRIBUT MÃ‰TIER VALIDÃ‰: {column_name.upper()}

ğŸ” ANALYSE AUTOMATIQUE:
â€¢ Types d'entitÃ©s dÃ©tectÃ©es: {', '.join(entity_types) if entity_types else 'Aucune entitÃ© spÃ©cifique'}
â€¢ Nombre total d'entitÃ©s: {total_entities}
â€¢ Niveau de sensibilitÃ©: {sensitivity}

ğŸ“‹ CLASSIFICATION RGPD:
â€¢ CatÃ©gorie: {rgpd_category}
â€¢ Politique Ranger recommandÃ©e: {metadata.get('recommended_ranger_policy', 'Non dÃ©finie')}

âœ… VALIDATION:
â€¢ Statut: ValidÃ© par Data Steward
â€¢ Date de validation: {datetime.now().strftime('%Y-%m-%d')}

ğŸ“Š Ã‰CHANTILLONS:
{self._format_sample_values(metadata.get('sample_values', []))}
        """.strip()
        
        return description

    def _format_sample_values(self, sample_values):
        """Formater les valeurs d'Ã©chantillon pour la description"""
        if not sample_values:
            return "â€¢ Aucun Ã©chantillon disponible"
        
        formatted_samples = []
        for i, sample in enumerate(sample_values[:3], 1):  # Limiter Ã  3 Ã©chantillons
            # Masquer partiellement pour la confidentialitÃ©
            if len(sample) > 10:
                masked_sample = sample[:3] + "***" + sample[-2:]
            else:
                masked_sample = sample[:2] + "***"
            formatted_samples.append(f"â€¢ Ã‰chantillon {i}: {masked_sample}")
        
        return '\n'.join(formatted_samples)

    def create_data_lineage_entities(self):
        """CrÃ©er des entitÃ©s pour la traÃ§abilitÃ© des donnÃ©es"""
        # Cette fonction pourrait Ãªtre Ã©tendue pour crÃ©er des entitÃ©s Atlas
        # reprÃ©sentant les datasets, tables, et leurs relations
        logger.info("ğŸ”— Fonction de lignage des donnÃ©es - Ã€ implÃ©menter selon vos besoins")
        
    def sync_governance_metadata(self, preview_only=False):
        """Fonction principale de synchronisation pour la gouvernance"""
        logger.info("ğŸš€ DÃ©but de la synchronisation mÃ©tadonnÃ©es gouvernance")
        
        try:
            # PrÃ©visualisation obligatoire
            preview = self.preview_sync_data()
            
            if preview_only:
                logger.info("ğŸ‘ï¸  Mode prÃ©visualisation uniquement - Aucune modification dans Atlas")
                return {"success": True, "preview": preview, "sync_executed": False}
            
            if not preview["will_sync"]:
                logger.warning("ğŸ›‘ ArrÃªt: Aucune mÃ©tadonnÃ©e validÃ©e Ã  synchroniser")
                return {"success": False, "error": "Aucune mÃ©tadonnÃ©e validÃ©e", "preview": preview}
            
            # Demander confirmation
            if not self._confirm_sync(preview):
                logger.info("ğŸ›‘ Synchronisation annulÃ©e par l'utilisateur")
                return {"success": False, "error": "AnnulÃ©e par l'utilisateur", "preview": preview}
            # Demander confirmation
            if not self._confirm_sync(preview):
                logger.info("ğŸ›‘ Synchronisation annulÃ©e par l'utilisateur")
                return {"success": False, "error": "AnnulÃ©e par l'utilisateur", "preview": preview}
            
            # Sauvegarder l'Ã©tat actuel d'Atlas (optionnel)
            self._backup_atlas_state()
            
            logger.info("â–¶ï¸  EXÃ‰CUTION DE LA SYNCHRONISATION...")
            
            # 1. CrÃ©er les classifications de sensibilitÃ©
            if not self.create_sensitivity_classifications():
                logger.error("Ã‰chec crÃ©ation classifications")
                return False
            
            # 2. CrÃ©er le glossaire mÃ©tier
            glossary_guid = self.create_business_glossary()
            if not glossary_guid:
                logger.error("Ã‰chec crÃ©ation glossaire")
                return False
            
            # 3. CrÃ©er les catÃ©gories RGPD basÃ©es sur les donnÃ©es rÃ©elles
            category_guids = self.create_rgpd_categories(glossary_guid)
            
            # 4. Synchroniser uniquement les mÃ©tadonnÃ©es validÃ©es
            synced_terms = self.create_validated_metadata_terms(glossary_guid, category_guids)
            
            # 5. CrÃ©er la lignage (optionnel)
            self.create_data_lineage_entities()
            # 6. Assigner automatiquement les termes aux colonnes Hive  
            assignment_result = self.automate_hive_glossary_assignment("entites_marocaines")  
   
            result = {  
             "success": True,  
             "glossary_guid": glossary_guid,  
             "validated_terms_synced": synced_terms,  
             "categories_created": len(category_guids),  
             "sync_timestamp": datetime.now().isoformat(),  
             "preview": preview  
            }  
  





  
            result.update({  
              "hive_assignment": assignment_result  
            })
            
            # Marquer les mÃ©tadonnÃ©es comme synchronisÃ©es
            self._mark_as_synced(synced_terms)
            
            result = {
                "success": True,
                "glossary_guid": glossary_guid,
                "validated_terms_synced": synced_terms,
                "categories_created": len(category_guids),
                "sync_timestamp": datetime.now().isoformat(),
                "preview": preview
            }
            
            logger.info(f"âœ… Synchronisation terminÃ©e avec succÃ¨s: {synced_terms} termes validÃ©s")
            return result
            
        except Exception as e:
            logger.error(f"âŒ Erreur lors de la synchronisation: {str(e)}")
            return {"success": False, "error": str(e)}
        
        finally:
            self.mongo_client.close()

    def _confirm_sync(self, preview):
        """Demander confirmation avant synchronisation"""
        print("\n" + "="*60)
        print("âš ï¸  CONFIRMATION REQUISE")
        print("="*60)
        print(f"Vous allez synchroniser {preview['validated_metadata']} mÃ©tadonnÃ©es validÃ©es vers Atlas")
        print(f"CatÃ©gories RGPD: {', '.join(preview['rgpd_categories'])}")
        print(f"Niveaux de sensibilitÃ©: {', '.join(preview['sensitivity_levels'])}")
        print("\nâš ï¸  Cette opÃ©ration va crÃ©er/modifier des Ã©lÃ©ments dans Apache Atlas")
        
        response = input("\nğŸ¤” Continuer la synchronisation? (oui/non): ").lower().strip()
        return response in ['oui', 'o', 'yes', 'y']
    
    def _backup_atlas_state(self):
        """Sauvegarder l'Ã©tat actuel d'Atlas (optionnel)"""
        logger.info("ğŸ’¾ Sauvegarde de l'Ã©tat Atlas (recommandÃ©)")
        # ImplÃ©mentation optionnelle pour sauvegarder l'Ã©tat actuel
        pass
    
    def _mark_as_synced(self, synced_count):
        """Marquer les mÃ©tadonnÃ©es synchronisÃ©es"""
        if synced_count > 0:
            enriched_metadata = self.metadata_db['enriched_metadata']
            enriched_metadata.update_many(
                {"validation_status": "validated"},
                {"$set": {"atlas_sync_status": "synced", "atlas_sync_date": datetime.now()}}
            )
            logger.info(f"ğŸ“ {synced_count} mÃ©tadonnÃ©es marquÃ©es comme synchronisÃ©es")

def main():
    """Point d'entrÃ©e principal"""
    governance = AtlasMetadataGovernance()
    result = governance.sync_governance_metadata()
    
    print("\n" + "="*60)
    print("RÃ‰SULTAT DE LA SYNCHRONISATION GOUVERNANCE")
    print("="*60)
    print(json.dumps(result, indent=2, ensure_ascii=False))
    
    if result.get("success"):
        print(f"\nâœ… Synchronisation rÃ©ussie!")
        print(f"ğŸ“š Termes validÃ©s synchronisÃ©s: {result.get('validated_terms_synced', 0)}")
        print(f"ğŸ“‚ CatÃ©gories RGPD crÃ©Ã©es: {result.get('categories_created', 0)}")
        print(f"ğŸ†” GUID du glossaire: {result.get('glossary_guid')}")
    else:
        print(f"\nâŒ Ã‰chec de la synchronisation: {result.get('error')}")

if __name__ == "__main__":
    main()